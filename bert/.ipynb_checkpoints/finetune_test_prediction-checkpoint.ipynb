{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "#from load import *\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "#from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "#                              TensorDataset)\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "\n",
    "# push the model to GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bert pre-trained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "#load the bert tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      #print (cls_hs)\n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = os.path.join(models_dir, model_name)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval() #so dropout is stopped during prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New to vinyl. I don't even have a record playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Trying to figure out what movie to watch on Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Teleportation Tourette's Syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>What are some things that people don't think i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Scariest horror sci-fi movie you've ever seen?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                              texts\n",
       "0       0  New to vinyl. I don't even have a record playe...\n",
       "1       0  Trying to figure out what movie to watch on Fr...\n",
       "2       0                  Teleportation Tourette's Syndrome\n",
       "3       0  What are some things that people don't think i...\n",
       "4       0  Scariest horror sci-fi movie you've ever seen?..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year=2017\n",
    "ml = \"bert\"\n",
    "pretrained = \"base_uncased\"\n",
    "dist = \"natural\" #balanced\n",
    "model_name = \"saved_weights_\"+dist+\".pt\"\n",
    "model_state = \"finetuned\" #pretrained\n",
    "\n",
    "root_dir = \"/projets/sig/mullah/nlp/depression\"\n",
    "processed_test_dir = os.path.join(root_dir, \"data\", \"processed\", str(year), \"test\", \"cumulative\")\n",
    "models_dir = os.path.join(root_dir, \"models\", str(year), ml, pretrained, model_state)\n",
    "chunk=10\n",
    "#chunks = [i for i in range(1, 2)]\n",
    "#for chunk in chunks:\n",
    "#    processed_chunk_test_dir = os.path.join(processed_test_dir, \"chunk \"+str(chunk))\n",
    "user = 'test_subject6208'\n",
    "user_processed_chunk_test_file_path = os.path.join(processed_test_dir, \"chunk \"+str(chunk), \"test_subject6208.csv\")    \n",
    "df = pd.read_csv(user_processed_chunk_test_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df[\"texts\"]\n",
    "test_labels = df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 128,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "batch_size = 32\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.313737   0.22653219 0.14109188 0.19103979 0.18363595 0.3683633\n",
      " 0.07018864 0.08468268 0.21550453 0.10773145 0.10639189 0.11175684\n",
      " 0.09720282 0.07875136 0.18316835 0.06987192 0.18239741 0.1595244\n",
      " 0.2267296  0.11331133 0.13026032 0.1735904  0.07516471 0.12994662\n",
      " 0.13969785 0.24234377 0.29262412 0.12700634 0.19589216 0.1596138\n",
      " 0.1071743  0.13622814]\n",
      "[0.3471226  0.22769818 0.27624613 0.04488092 0.16681482 0.13931377\n",
      " 0.08233314 0.32486337 0.07587463 0.10999674 0.23022294 0.23022294\n",
      " 0.49643373 0.23065124 0.16948253 0.05861456 0.28142595 0.22763345\n",
      " 0.14582743 0.4115311  0.38368538 0.19332324 0.3099845  0.13341728\n",
      " 0.23552547 0.10780931 0.14743225 0.3457684  0.15506501 0.19951825\n",
      " 0.5963022  0.14699031]\n",
      "[0.16766287 0.09607138 0.25689596 0.13154837 0.13154837 0.24548526\n",
      " 0.08345157 0.06193476 0.41575265 0.0827951  0.10645708 0.14552036\n",
      " 0.12894285 0.18541175 0.23020132 0.09108156 0.09558577 0.14917755\n",
      " 0.11894547 0.14734621 0.13987766 0.03178709 0.12236711 0.08069683\n",
      " 0.10033381 0.06338978 0.12882714 0.28941467 0.09490187 0.07127279\n",
      " 0.04180969 0.14919968]\n",
      "[0.20673926 0.1876325  0.33098748 0.19188581 0.14355002 0.1711637\n",
      " 0.15149736 0.18651292 0.13497412 0.11230265 0.13898271 0.26931176\n",
      " 0.09723198 0.13508974 0.23778641 0.0317322  0.26384512 0.05013423\n",
      " 0.31885043 0.08535229 0.40747762 0.41143936 0.18462053 0.1475307\n",
      " 0.08204273 0.12704356 0.08975146 0.1490409  0.1147634  0.19088683\n",
      " 0.05863267 0.10849955]\n",
      "[0.1493351  0.19220941 0.08031261 0.08373975 0.16452508 0.17328915\n",
      " 0.07525515 0.1794795  0.12678517 0.13645199 0.15691702 0.15866716\n",
      " 0.19734903 0.4434634  0.4294053  0.2279384  0.10461672 0.11133268\n",
      " 0.22707048 0.15161678 0.06726335 0.10563032 0.09219832 0.23037252\n",
      " 0.08265865 0.12943871 0.10543381 0.1719344  0.09352031 0.13084278\n",
      " 0.12843974 0.12665634]\n",
      "[0.05959302 0.11123865 0.17684071 0.08432271 0.16539906 0.08612325\n",
      " 0.14748053 0.14069739 0.08165863 0.09865681 0.03426453 0.17574945\n",
      " 0.13684765 0.21543671 0.04498168 0.09626448 0.14003234 0.10594417\n",
      " 0.1032216  0.05076287 0.13798308 0.15307915 0.04482344 0.10360502\n",
      " 0.14906499 0.2790186  0.10271005 0.2945824  0.13624848 0.1144421\n",
      " 0.04992153 0.07749376]\n",
      "[0.33827615 0.19067039 0.15711471 0.1906613  0.10721719 0.21442932\n",
      " 0.07634559 0.12291291 0.12408648 0.15683353 0.09675074 0.07067735\n",
      " 0.218532   0.20146634 0.14026213 0.1087315  0.2129207  0.10819595\n",
      " 0.10148114 0.10148114 0.16691048 0.21720189 0.22500777 0.07617837\n",
      " 0.24714014 0.07069375 0.16820233 0.24109292 0.17706585 0.07033059\n",
      " 0.10459945 0.1025768 ]\n",
      "[0.18077621 0.20850192 0.09048079 0.19410019 0.1577804  0.11394898\n",
      " 0.17996639 0.10199607 0.14102842 0.0868664  0.40583396 0.16088745\n",
      " 0.15214461 0.1603141  0.22950204 0.15299158 0.11570327 0.16818465\n",
      " 0.10878832 0.11208112 0.09230972 0.19460137 0.21390155 0.11780357\n",
      " 0.10375984 0.31189248 0.347706   0.16454591 0.07156361 0.26839113\n",
      " 0.32279179 0.13603567]\n",
      "[0.09694726 0.23216534 0.11430906 0.22294603 0.17269978 0.04496573\n",
      " 0.08607283 0.15128802 0.1558317  0.07904972 0.26623657 0.14786059\n",
      " 0.55979186 0.10593614 0.09324063 0.23120503 0.11881294 0.29500622\n",
      " 0.14525543 0.21668027 0.15153445 0.11989887 0.30701703 0.07127941\n",
      " 0.22170903 0.41479704 0.05186151 0.09553537 0.15177321 0.06055285\n",
      " 0.32791504 0.15866823]\n",
      "[0.3611527  0.335569   0.1544406  0.25105217 0.2217734  0.11995351\n",
      " 0.16016309 0.12425251 0.07346743 0.07279629 0.09389771 0.08992716\n",
      " 0.11361482 0.06073959 0.17592    0.25411385 0.12733011 0.36522168\n",
      " 0.13236494 0.1730822  0.07770986 0.10531545 0.12529187 0.16131143\n",
      " 0.16820982 0.09447317 0.03307005 0.20794621 0.06354284 0.07088955\n",
      " 0.08569713 0.25049132]\n",
      "[0.11006742 0.0652983  0.24936904 0.08642074 0.17335458 0.16197\n",
      " 0.24394795 0.07924636 0.07082407 0.1690204  0.11688124 0.18064466\n",
      " 0.05710852 0.09387068 0.13485962 0.167912   0.10085925 0.12550336\n",
      " 0.18517673 0.15737262 0.45086178 0.32071972 0.20597021 0.23658398\n",
      " 0.14557926 0.17036295 0.11249069 0.11174877 0.11118273 0.13844234\n",
      " 0.1080522  0.11688439]\n",
      "[0.29022482 0.2681869  0.09875125 0.21539077 0.33687854 0.14878374\n",
      " 0.13187689 0.10832986 0.33958358 0.16183183 0.08362132 0.18784444\n",
      " 0.31590644 0.38931802 0.13279113 0.20673095 0.09103826 0.1852641\n",
      " 0.12321472 0.06033081 0.1878893  0.09554835 0.12280254 0.11172374\n",
      " 0.12729776 0.13542067 0.13475762 0.03387536 0.07916307 0.09812292\n",
      " 0.14684926 0.13313623]\n",
      "[0.25068128 0.09847391 0.11890554 0.2165576  0.12616597 0.15255085\n",
      " 0.18361636 0.15545546 0.09492269 0.15210237 0.06353668 0.07476314\n",
      " 0.12962191 0.27660763 0.10285944 0.24119243 0.15773001 0.11048656\n",
      " 0.27422273 0.15216412 0.1415744  0.10568246 0.22428674 0.2584706\n",
      " 0.15991344 0.27446166 0.5271783  0.27429762 0.14972764 0.10426545\n",
      " 0.05019569 0.05481753]\n",
      "[0.12920536 0.1479933  0.08641376 0.2689472  0.19981022 0.28952023\n",
      " 0.14855905 0.10631714 0.2911805  0.24469745 0.08455388 0.10485848\n",
      " 0.17186005 0.10292915 0.17091541 0.1605772  0.10292915 0.19935198\n",
      " 0.10837378 0.29991263 0.16052137 0.0832995  0.10446621 0.08276824\n",
      " 0.1464806  0.14917192 0.14122014 0.10625126 0.4898452  0.09024367\n",
      " 0.13272004 0.24441908]\n",
      "[0.14910576 0.15777098 0.07952372 0.17452374 0.1417486  0.09923667\n",
      " 0.30156565 0.1900431  0.15557827 0.05903843 0.2570335  0.05569348\n",
      " 0.10846836 0.11386622 0.03908947 0.15583585 0.12477963 0.17291957\n",
      " 0.08436227 0.37027055 0.06185379 0.18499327 0.11889894 0.15361162\n",
      " 0.12925892 0.07054197 0.15407808 0.28506657 0.1708732  0.25431752\n",
      " 0.12784271 0.37919265]\n",
      "[0.09574258 0.2548513  0.39074045 0.08853858 0.24161536 0.07650869\n",
      " 0.07318759 0.063117   0.07776926 0.08594251 0.09932305 0.13686846\n",
      " 0.09623252 0.08738913 0.10550578 0.11177395 0.13487495 0.15765472\n",
      " 0.17580582 0.17013898 0.12653849 0.23368031 0.223987   0.28489235\n",
      " 0.05859797 0.11633954 0.13004632 0.23629007 0.33249047 0.22341542\n",
      " 0.07052676 0.15673715]\n",
      "[0.13548122 0.10927226 0.46574467 0.24587683 0.2588589  0.06855262\n",
      " 0.13201822 0.14143586 0.14428069 0.11256984 0.05663611 0.11615767\n",
      " 0.11236996 0.09204218 0.04358338 0.13577843 0.11767    0.12848935\n",
      " 0.10200155 0.09320609 0.10610884 0.12054149 0.16112058 0.5664835\n",
      " 0.45061383 0.10644462 0.11265611 0.20491827 0.12642343 0.3014713\n",
      " 0.10674163 0.23998024]\n",
      "[0.11403207 0.0620111  0.13543427 0.12768875 0.42897722 0.19591427\n",
      " 0.12446289 0.09072608 0.22142594 0.05671459 0.1106126  0.16019098\n",
      " 0.02648779 0.13889429 0.12503843 0.14426088 0.10859544 0.15742554\n",
      " 0.08677315 0.1503325  0.17541489 0.20875022 0.15320267 0.10591132\n",
      " 0.10032338 0.23360536 0.12386388 0.07302012 0.08296074 0.12136673\n",
      " 0.125165   0.12171844]\n",
      "[0.1879351  0.16437247 0.27701065 0.2617025  0.15847717 0.05258461\n",
      " 0.09165058 0.3825586  0.08790765 0.22813758 0.05313981 0.2991196\n",
      " 0.20392871 0.11356588 0.12397805 0.20153745 0.30860972 0.15830109\n",
      " 0.20759624 0.20165747 0.20925781 0.1295249  0.04709964 0.10545115\n",
      " 0.07395148 0.18251331 0.05611621 0.18156382 0.15697381 0.0579041\n",
      " 0.1514058  0.22662567]\n",
      "[0.1543042  0.23294272 0.400455   0.1770915  0.19918159 0.08645794\n",
      " 0.13316038 0.14077535 0.12363286 0.10845336 0.10745151 0.04601184\n",
      " 0.1403618  0.23401001 0.16526683 0.1804933  0.29267168 0.04603177\n",
      " 0.08748093 0.08748093 0.24247295 0.16848437 0.18121848 0.10299372\n",
      " 0.12608229 0.12819768 0.08889624 0.09925409 0.19331177 0.09311256\n",
      " 0.07543952 0.04502212]\n",
      "[0.08555966 0.06991358 0.09120347 0.08865391 0.108756   0.04714875\n",
      " 0.09313732 0.15276925 0.11011674 0.12014158 0.28196177 0.16560599\n",
      " 0.3382993  0.12988195 0.10826764 0.07244181 0.08666766 0.20383431\n",
      " 0.18356943 0.12639719 0.08847797 0.07500073 0.06944024 0.10883589\n",
      " 0.2010424  0.311054   0.10910628 0.04885482 0.10042273 0.14594004\n",
      " 0.0745042  0.07330239]\n",
      "[0.12083104 0.06417202 0.12083104 0.19903311 0.18051344 0.14226134\n",
      " 0.0680007  0.28537333 0.08807945 0.31272593 0.24532865 0.26712537\n",
      " 0.29217204 0.3564662  0.23498422 0.15362592 0.1703803  0.1659183\n",
      " 0.176165   0.08655494 0.22743575 0.1208443  0.02990813 0.11273021\n",
      " 0.20398888 0.22229753 0.2955012  0.09193899 0.06073727 0.08787005\n",
      " 0.08407307 0.18923315]\n",
      "[0.201367   0.04579684 0.11932797 0.12527451 0.14116032 0.20489767\n",
      " 0.10269617 0.2830313  0.16937152 0.13629325 0.20250526 0.13572311\n",
      " 0.2010894  0.08451614 0.09228785 0.25490743 0.13909596 0.0909305\n",
      " 0.4452441  0.23925708 0.09631017 0.07680977 0.19986606 0.15459888\n",
      " 0.21196926 0.02856948 0.3268302  0.19913498 0.11128101 0.17337199\n",
      " 0.0933002  0.07358135]\n",
      "[0.1296806  0.10817327 0.21582888 0.0434101  0.1356957  0.17485902\n",
      " 0.16162613 0.08900968 0.12913708 0.14879267 0.1109396  0.1322715\n",
      " 0.25754544 0.15222052 0.13590865 0.05336622 0.11188655 0.0596659\n",
      " 0.1293918  0.2733778  0.05374726 0.12314183 0.09515922 0.20888934\n",
      " 0.09473336 0.0917838  0.19896504 0.2534523  0.19040933 0.29105824\n",
      " 0.14810744 0.0760717 ]\n",
      "[0.08738098 0.21870235 0.14304537 0.15287401 0.10461219 0.06761834\n",
      " 0.19343868 0.10662661 0.13191445 0.12488579 0.30929706 0.07182732\n",
      " 0.23273295 0.16169767 0.16572797 0.16040146 0.05257031 0.10777926\n",
      " 0.08644149 0.2637899  0.09768764 0.22782713 0.21593744 0.12848844\n",
      " 0.22674525 0.12327061 0.27329606 0.26493832 0.29732662 0.10012593\n",
      " 0.25201663 0.07085322]\n",
      "[0.18831699 0.05470137 0.3964485  0.08265394 0.27347118 0.11955763\n",
      " 0.08481939 0.41232583 0.561618   0.17709884 0.15466675 0.09769896\n",
      " 0.15527551 0.5859028  0.40278414 0.08096328 0.11257008 0.25281313\n",
      " 0.20835006 0.08092427 0.47921395 0.28492332 0.17507608 0.16889286\n",
      " 0.07060895 0.10974718 0.12563631 0.08980253 0.08990797 0.17667773\n",
      " 0.09670507 0.07967149]\n",
      "[0.23576616 0.13722907 0.11087141 0.2968138  0.23646472 0.13188139\n",
      " 0.2512602  0.15061003 0.1721826  0.1992489  0.08911959 0.12550902\n",
      " 0.13021834 0.12013728 0.3410868  0.24178232 0.1369471  0.1958812\n",
      " 0.06960491 0.36442065 0.19453953 0.07055144 0.07353294 0.20399572\n",
      " 0.2437213  0.05886942 0.16511671 0.1743629  0.20262994 0.07337492\n",
      " 0.1440552  0.29592472]\n",
      "[0.24517235 0.16005866 0.3446218  0.25949532 0.08556166 0.12272422\n",
      " 0.18842256 0.17228827 0.12932445 0.07375959 0.31984115 0.08033291\n",
      " 0.1860379  0.15004347 0.13205266 0.10156017 0.07409547 0.06793974\n",
      " 0.2833249  0.18042189 0.14295211 0.15214528 0.15084106 0.14276212\n",
      " 0.15304871 0.09401976 0.10489824 0.15359978 0.17651823 0.09249996\n",
      " 0.41808504 0.19232485]\n",
      "[0.1812598  0.19991876 0.24942173 0.06882738 0.21560417 0.2722034\n",
      " 0.1667586  0.07156099 0.25157002 0.11702017 0.09364679 0.17244734\n",
      " 0.22341563 0.2424723  0.06725115 0.07573052 0.11527318 0.26215625\n",
      " 0.14731379 0.08000539 0.24522144 0.23398171 0.09565546 0.12168292\n",
      " 0.0989483  0.04666465 0.2875653  0.0893349  0.09355463 0.0745955\n",
      " 0.24857439 0.13615213]\n",
      "[0.07352275 0.17802587 0.08974754 0.08256915 0.10720919 0.12084671\n",
      " 0.19506158 0.13583541 0.11026746 0.13790387 0.4042908  0.2418982\n",
      " 0.46952954 0.22982287 0.0836949  0.07537445 0.10432429 0.14428589\n",
      " 0.05268729 0.07022727 0.2392553  0.12601599 0.15425803 0.18615717\n",
      " 0.11243255 0.13735235 0.0392244  0.05361781 0.11500789 0.06649657\n",
      " 0.05525776 0.09213758]\n",
      "[0.10882474 0.16114601 0.17651112 0.08588353 0.2335538  0.23587811\n",
      " 0.24702536 0.10268133 0.07742726 0.122062   0.3554228  0.14428999\n",
      " 0.14760493 0.06964642 0.42716587 0.24240851 0.14621988 0.11695029\n",
      " 0.19198269 0.23199609 0.30345288 0.09821098 0.11764038 0.0983941\n",
      " 0.13805789 0.08597158 0.1773501  0.11862864 0.16268446 0.17715356\n",
      " 0.05634674 0.12685989]\n",
      "[0.07552072 0.11295956 0.21897243 0.06191926 0.05386168 0.13343124\n",
      " 0.07184246 0.24646497 0.12761196 0.14240026 0.20268278 0.0884401\n",
      " 0.20509617 0.1031417  0.2981931  0.24907275 0.07678029 0.28113008\n",
      " 0.1101275  0.14182784 0.16096482 0.0922275  0.10458765 0.09495128\n",
      " 0.2385788  0.2100435  0.1000448  0.20791967 0.1209518  0.3350045\n",
      " 0.2013955  0.12067606]\n",
      "[0.2288045  0.36154765 0.14095981 0.18864985 0.1695198  0.08947868\n",
      " 0.10724283 0.30840835 0.13154197 0.20107669 0.10533858 0.10618959\n",
      " 0.4988863  0.3411665  0.13629709 0.05168094 0.17727041 0.1649136\n",
      " 0.09652397 0.29134193 0.17618273 0.15692292 0.29667994 0.12293112\n",
      " 0.12797652 0.22616151 0.10426585 0.36576894 0.20207277 0.18194756\n",
      " 0.11437713 0.03726364]\n",
      "[0.21589094 0.3279235  0.32324678 0.10393397 0.05880995 0.05061283\n",
      " 0.16347487 0.08771654 0.21610199 0.11112025 0.04128053 0.11304786\n",
      " 0.10806851 0.3688549  0.17593658 0.28066957 0.23733371 0.07627087\n",
      " 0.08106968 0.08811278 0.2618428  0.13670386 0.08724041 0.13346983\n",
      " 0.12904319 0.07934278 0.05710785 0.05944162 0.1469954  0.12598519\n",
      " 0.13643917 0.12502018]\n",
      "[0.07418681 0.09702098 0.04545347 0.06214093 0.3305416  0.5412816\n",
      " 0.17115687 0.10910222 0.13369162 0.33261877 0.21098484 0.38439122\n",
      " 0.2684129  0.08530948 0.10147298 0.21276979 0.18904132 0.19476901\n",
      " 0.06701728 0.0501367  0.15812014 0.26456583 0.0952149  0.10860644\n",
      " 0.5413066  0.05624256 0.12699695 0.18416286 0.2077359  0.20424694\n",
      " 0.12311056 0.2525863 ]\n",
      "[0.12628436 0.12053922 0.09453894 0.09972529 0.06071542 0.13359874\n",
      " 0.12661749 0.29363745 0.12363118 0.18023561 0.05220475 0.21097669\n",
      " 0.06780293 0.12987635 0.10651415 0.12359877 0.27672914 0.05830569\n",
      " 0.1329466  0.3019686  0.05048458 0.11808538 0.09150504 0.17997389\n",
      " 0.09794272 0.1734791  0.14343439 0.2633756  0.21412513 0.0789409\n",
      " 0.08368625 0.12900098]\n",
      "[0.30566615 0.13742419 0.1138458  0.11038383 0.20013136 0.11346916\n",
      " 0.1070412  0.3336879  0.06417652 0.1261118  0.0928189  0.13664328\n",
      " 0.13984987 0.1861543  0.18854402 0.07859197 0.11881045 0.10964644\n",
      " 0.09262493 0.16257982 0.27662134 0.25686106 0.16498083 0.16985856\n",
      " 0.33686277 0.3823682  0.10180102 0.06021661 0.22138384 0.20692854\n",
      " 0.21550933 0.20614445]\n",
      "[0.0486272  0.18842834 0.08923068 0.19043897 0.1350893  0.18350758\n",
      " 0.1126021  0.16134062 0.16304012 0.11709285 0.1946511  0.23683313\n",
      " 0.06156483 0.58125997 0.10226772 0.3168343  0.09436879 0.07205114\n",
      " 0.22258109 0.19960453 0.06088779 0.13340166 0.21607958 0.17777108\n",
      " 0.17071807 0.35927594 0.17086247 0.4398082  0.09969044 0.26050794\n",
      " 0.09987424 0.19716404]\n",
      "[0.16239707 0.09262516 0.15801415 0.11493139 0.10665168 0.13624245\n",
      " 0.25347328 0.14803806 0.23246491 0.21869572 0.35101032 0.13704045\n",
      " 0.16759998 0.286896   0.1326723  0.10969186 0.09822157 0.10199977\n",
      " 0.17253524 0.35528484 0.18512857 0.21593864 0.1473605  0.23116353\n",
      " 0.11161392 0.23231687 0.15606695 0.14268476 0.23029216 0.18479526\n",
      " 0.09803437 0.14734143]\n",
      "[0.19720122 0.11469967 0.26761755 0.19658962 0.11470759 0.09935986\n",
      " 0.16253912 0.18498935 0.31950107 0.21820857 0.2925725  0.0852477\n",
      " 0.08081223 0.08230425 0.13963601 0.05698027 0.06381734 0.13837177\n",
      " 0.22198088 0.0787567  0.15098545 0.10132527 0.12093528 0.1455169\n",
      " 0.21341333 0.22984496 0.15786576 0.07296178 0.04287353 0.09154901\n",
      " 0.07703814 0.05822487]\n",
      "[0.22459272 0.12234651 0.09913222 0.3679142  0.16738416 0.09382392\n",
      " 0.13152987 0.06087125 0.17289017 0.11770003 0.12895815 0.0803575\n",
      " 0.2226077  0.04320895 0.08742145 0.11219317 0.10838442 0.17465149\n",
      " 0.14116916 0.10708845 0.30500305 0.1181621  0.15292186 0.12151805\n",
      " 0.14468102 0.15082842 0.10811008 0.17804025 0.29396904 0.15338683\n",
      " 0.087979   0.2648897 ]\n",
      "[0.16107118 0.09000326 0.20708854 0.10362029 0.24408855 0.08842976\n",
      " 0.14981711 0.28301644 0.09018906 0.132475   0.29035825 0.16519348\n",
      " 0.16145545 0.09144535 0.07442299 0.14670046 0.06877892 0.19482122\n",
      " 0.17464875 0.33936697 0.10698374 0.14153078 0.1428322  0.18255219\n",
      " 0.09705954 0.23968634 0.07702593 0.35495275 0.18553436 0.09528453\n",
      " 0.13895623 0.09981591]\n",
      "[0.16019198 0.16931958 0.05885978 0.08701295 0.1076117  0.32986265\n",
      " 0.17595255 0.17117761 0.16357583 0.37339067 0.13298845 0.18534598\n",
      " 0.22568502 0.13794659 0.12258452 0.22671312 0.08098606 0.0724505\n",
      " 0.18589725 0.21640557 0.17319784 0.3967954  0.06917298 0.0891882\n",
      " 0.29064235 0.39108294 0.28723714 0.10661703 0.12482616 0.09731448\n",
      " 0.09755898 0.05639911]\n",
      "[0.12429976 0.09267274 0.12764049 0.11460912 0.13827324 0.15707749\n",
      " 0.19658372 0.1672159  0.04645188 0.04500468 0.14895716 0.12700135\n",
      " 0.09903246 0.11592671 0.14983845 0.10861287 0.20407088 0.11862098\n",
      " 0.06787744 0.13602121 0.09555238 0.20647466 0.14552799 0.11342604\n",
      " 0.18620004 0.13896994 0.24778925 0.1313593  0.19845556 0.09712707\n",
      " 0.14166713 0.08655407]\n",
      "[0.10571333 0.1139452  0.165037   0.06396253 0.15148619 0.13442932\n",
      " 0.10442584 0.1024456  0.12317835 0.23006596 0.13828896 0.10465675\n",
      " 0.08361948 0.14841141 0.2618594  0.13020337 0.12577465 0.09397035\n",
      " 0.11802948 0.10102905 0.06940986 0.21524364 0.07356042 0.25374997\n",
      " 0.06681348 0.1214008  0.18077412 0.09721404 0.17675029 0.19894257\n",
      " 0.09761646 0.09149048]\n",
      "[0.15877204 0.25641555 0.26257142 0.15921444 0.09837516 0.1999502\n",
      " 0.35229576 0.19876648 0.14615063 0.26829025 0.09532673 0.10618782\n",
      " 0.09649988 0.18882269 0.35506845 0.08470877 0.09067802 0.14841713\n",
      " 0.15699415 0.07445747 0.06634368 0.23185053 0.19196975 0.18302318\n",
      " 0.08863557 0.25895938 0.21155326 0.11380886 0.15501185 0.18808383\n",
      " 0.09901409 0.12661918]\n",
      "[0.11618201 0.06869565 0.0886372  0.33063498 0.06348969 0.11897966\n",
      " 0.1697188  0.09810489 0.21140672 0.20997445 0.0595712  0.08976044\n",
      " 0.19587651 0.10376422 0.13250615 0.18658069 0.08980253 0.26053935\n",
      " 0.15163867 0.14693557 0.1360196  0.09644145 0.21100053 0.1167632\n",
      " 0.127366   0.1433325  0.16177204 0.08290879 0.09739456 0.12674809\n",
      " 0.12178514 0.19502778]\n",
      "[0.16045909 0.1439079  0.32609677 0.07358496 0.32616255 0.17255211\n",
      " 0.2036105  0.10912842 0.06742226 0.13344641 0.15549216 0.1518359\n",
      " 0.13473491 0.10725901 0.15310082 0.08849658 0.08443678 0.08952818\n",
      " 0.19079274 0.10283087 0.13770373 0.07230391 0.06319531 0.14005142\n",
      " 0.14499322 0.15914038 0.10568187 0.11125638 0.10133632]\n",
      "[0.313737, 0.22653219, 0.14109188, 0.19103979, 0.18363595, 0.3683633, 0.07018864, 0.08468268, 0.21550453, 0.107731454, 0.10639189, 0.11175684, 0.09720282, 0.078751355, 0.18316835, 0.069871925, 0.18239741, 0.1595244, 0.2267296, 0.11331133, 0.13026032, 0.1735904, 0.075164706, 0.12994662, 0.13969785, 0.24234377, 0.29262412, 0.12700634, 0.19589216, 0.1596138, 0.1071743, 0.13622814, 0.3471226, 0.22769818, 0.27624613, 0.04488092, 0.16681482, 0.13931377, 0.08233314, 0.32486337, 0.07587463, 0.10999674, 0.23022294, 0.23022294, 0.49643373, 0.23065124, 0.16948253, 0.058614556, 0.28142595, 0.22763345, 0.14582743, 0.4115311, 0.38368538, 0.19332324, 0.3099845, 0.13341728, 0.23552547, 0.10780931, 0.14743225, 0.3457684, 0.15506501, 0.19951825, 0.5963022, 0.14699031, 0.16766287, 0.096071385, 0.25689596, 0.13154837, 0.13154837, 0.24548526, 0.08345157, 0.06193476, 0.41575265, 0.0827951, 0.106457084, 0.14552036, 0.12894285, 0.18541175, 0.23020132, 0.09108156, 0.09558577, 0.14917755, 0.11894547, 0.14734621, 0.13987766, 0.031787094, 0.12236711, 0.08069683, 0.10033381, 0.06338978, 0.12882714, 0.28941467, 0.09490187, 0.07127279, 0.041809693, 0.14919968, 0.20673926, 0.1876325, 0.33098748, 0.19188581, 0.14355002, 0.1711637, 0.15149736, 0.18651292, 0.13497412, 0.11230265, 0.13898271, 0.26931176, 0.097231984, 0.13508974, 0.23778641, 0.0317322, 0.26384512, 0.05013423, 0.31885043, 0.08535229, 0.40747762, 0.41143936, 0.18462053, 0.1475307, 0.08204273, 0.12704356, 0.08975146, 0.1490409, 0.1147634, 0.19088683, 0.058632668, 0.10849955, 0.1493351, 0.19220941, 0.08031261, 0.08373975, 0.16452508, 0.17328915, 0.07525515, 0.1794795, 0.12678517, 0.13645199, 0.15691702, 0.15866716, 0.19734903, 0.4434634, 0.4294053, 0.2279384, 0.104616724, 0.111332685, 0.22707048, 0.15161678, 0.06726335, 0.105630316, 0.09219832, 0.23037252, 0.08265865, 0.12943871, 0.10543381, 0.1719344, 0.093520306, 0.13084278, 0.12843974, 0.12665634, 0.05959302, 0.11123865, 0.17684071, 0.08432271, 0.16539906, 0.08612325, 0.14748053, 0.14069739, 0.08165863, 0.09865681, 0.034264535, 0.17574945, 0.13684765, 0.21543671, 0.04498168, 0.09626448, 0.14003234, 0.10594417, 0.1032216, 0.05076287, 0.13798308, 0.15307915, 0.044823438, 0.103605025, 0.14906499, 0.2790186, 0.102710046, 0.2945824, 0.13624848, 0.114442095, 0.049921535, 0.077493764, 0.33827615, 0.19067039, 0.15711471, 0.1906613, 0.10721719, 0.21442932, 0.07634559, 0.12291291, 0.12408648, 0.15683353, 0.096750736, 0.070677355, 0.218532, 0.20146634, 0.14026213, 0.1087315, 0.2129207, 0.108195946, 0.10148114, 0.10148114, 0.16691048, 0.21720189, 0.22500777, 0.07617837, 0.24714014, 0.070693746, 0.16820233, 0.24109292, 0.17706585, 0.07033059, 0.104599446, 0.1025768, 0.18077621, 0.20850192, 0.09048079, 0.19410019, 0.1577804, 0.11394898, 0.17996639, 0.10199607, 0.14102842, 0.0868664, 0.40583396, 0.16088745, 0.15214461, 0.1603141, 0.22950204, 0.15299158, 0.11570327, 0.16818465, 0.10878832, 0.11208112, 0.09230972, 0.19460137, 0.21390155, 0.117803566, 0.10375984, 0.31189248, 0.347706, 0.16454591, 0.07156361, 0.26839113, 0.32279179, 0.13603567, 0.09694726, 0.23216534, 0.11430906, 0.22294603, 0.17269978, 0.044965725, 0.08607283, 0.15128802, 0.1558317, 0.07904972, 0.26623657, 0.14786059, 0.55979186, 0.10593614, 0.09324063, 0.23120503, 0.11881294, 0.29500622, 0.14525543, 0.21668027, 0.15153445, 0.11989887, 0.30701703, 0.07127941, 0.22170903, 0.41479704, 0.051861506, 0.09553537, 0.15177321, 0.060552847, 0.32791504, 0.15866823, 0.3611527, 0.335569, 0.1544406, 0.25105217, 0.2217734, 0.11995351, 0.16016309, 0.124252506, 0.07346743, 0.07279629, 0.09389771, 0.08992716, 0.11361482, 0.060739588, 0.17592, 0.25411385, 0.12733011, 0.36522168, 0.13236494, 0.1730822, 0.07770986, 0.105315454, 0.12529187, 0.16131143, 0.16820982, 0.09447317, 0.03307005, 0.20794621, 0.063542835, 0.070889555, 0.08569713, 0.25049132, 0.11006742, 0.065298304, 0.24936904, 0.08642074, 0.17335458, 0.16197, 0.24394795, 0.079246365, 0.07082407, 0.1690204, 0.116881244, 0.18064466, 0.057108518, 0.093870685, 0.13485962, 0.167912, 0.100859255, 0.12550336, 0.18517673, 0.15737262, 0.45086178, 0.32071972, 0.20597021, 0.23658398, 0.14557926, 0.17036295, 0.11249069, 0.11174877, 0.111182734, 0.13844234, 0.1080522, 0.11688439, 0.29022482, 0.2681869, 0.09875125, 0.21539077, 0.33687854, 0.14878374, 0.13187689, 0.10832986, 0.33958358, 0.16183183, 0.08362132, 0.18784444, 0.31590644, 0.38931802, 0.13279113, 0.20673095, 0.091038264, 0.1852641, 0.12321472, 0.060330812, 0.1878893, 0.095548354, 0.12280254, 0.11172374, 0.12729776, 0.13542067, 0.13475762, 0.033875365, 0.079163074, 0.098122925, 0.14684926, 0.13313623, 0.25068128, 0.098473914, 0.118905544, 0.2165576, 0.12616597, 0.15255085, 0.18361636, 0.15545546, 0.09492269, 0.15210237, 0.06353668, 0.07476314, 0.12962191, 0.27660763, 0.10285944, 0.24119243, 0.15773001, 0.11048656, 0.27422273, 0.15216412, 0.1415744, 0.105682455, 0.22428674, 0.2584706, 0.15991344, 0.27446166, 0.5271783, 0.27429762, 0.14972764, 0.10426545, 0.050195694, 0.05481753, 0.12920536, 0.1479933, 0.086413756, 0.2689472, 0.19981022, 0.28952023, 0.14855905, 0.10631714, 0.2911805, 0.24469745, 0.08455388, 0.10485848, 0.17186005, 0.10292915, 0.17091541, 0.1605772, 0.10292915, 0.19935198, 0.108373776, 0.29991263, 0.16052137, 0.0832995, 0.104466215, 0.08276824, 0.1464806, 0.14917192, 0.14122014, 0.10625126, 0.4898452, 0.090243675, 0.13272004, 0.24441908, 0.14910576, 0.15777098, 0.07952372, 0.17452374, 0.1417486, 0.099236675, 0.30156565, 0.1900431, 0.15557827, 0.059038427, 0.2570335, 0.055693477, 0.10846836, 0.113866225, 0.03908947, 0.15583585, 0.12477963, 0.17291957, 0.08436227, 0.37027055, 0.06185379, 0.18499327, 0.11889894, 0.15361162, 0.12925892, 0.07054197, 0.15407808, 0.28506657, 0.1708732, 0.25431752, 0.12784271, 0.37919265, 0.095742576, 0.2548513, 0.39074045, 0.08853858, 0.24161536, 0.07650869, 0.07318759, 0.063117, 0.077769265, 0.08594251, 0.09932305, 0.13686846, 0.09623252, 0.08738913, 0.10550578, 0.11177395, 0.13487495, 0.15765472, 0.17580582, 0.17013898, 0.12653849, 0.23368031, 0.223987, 0.28489235, 0.05859797, 0.11633954, 0.13004632, 0.23629007, 0.33249047, 0.22341542, 0.070526764, 0.15673715, 0.13548122, 0.109272264, 0.46574467, 0.24587683, 0.2588589, 0.06855262, 0.13201822, 0.14143586, 0.14428069, 0.11256984, 0.05663611, 0.11615767, 0.11236996, 0.09204218, 0.043583382, 0.13577843, 0.11767, 0.12848935, 0.10200155, 0.09320609, 0.10610884, 0.12054149, 0.16112058, 0.5664835, 0.45061383, 0.10644462, 0.11265611, 0.20491827, 0.12642343, 0.3014713, 0.10674163, 0.23998024, 0.11403207, 0.062011104, 0.13543427, 0.12768875, 0.42897722, 0.19591427, 0.12446289, 0.09072608, 0.22142594, 0.056714587, 0.1106126, 0.16019098, 0.026487786, 0.13889429, 0.12503843, 0.14426088, 0.10859544, 0.15742554, 0.08677315, 0.1503325, 0.17541489, 0.20875022, 0.15320267, 0.10591132, 0.10032338, 0.23360536, 0.12386388, 0.07302012, 0.08296074, 0.12136673, 0.125165, 0.12171844, 0.1879351, 0.16437247, 0.27701065, 0.2617025, 0.15847717, 0.052584607, 0.09165058, 0.3825586, 0.08790765, 0.22813758, 0.053139806, 0.2991196, 0.20392871, 0.113565885, 0.12397805, 0.20153745, 0.30860972, 0.15830109, 0.20759624, 0.20165747, 0.20925781, 0.1295249, 0.047099635, 0.10545115, 0.07395148, 0.18251331, 0.05611621, 0.18156382, 0.15697381, 0.0579041, 0.1514058, 0.22662567, 0.1543042, 0.23294272, 0.400455, 0.1770915, 0.19918159, 0.08645794, 0.13316038, 0.14077535, 0.12363286, 0.108453356, 0.10745151, 0.046011843, 0.1403618, 0.23401001, 0.16526683, 0.1804933, 0.29267168, 0.04603177, 0.08748093, 0.08748093, 0.24247295, 0.16848437, 0.18121848, 0.10299372, 0.12608229, 0.12819768, 0.08889624, 0.09925409, 0.19331177, 0.09311256, 0.07543952, 0.04502212, 0.08555966, 0.06991358, 0.091203466, 0.08865391, 0.108756, 0.047148746, 0.09313732, 0.15276925, 0.110116735, 0.12014158, 0.28196177, 0.16560599, 0.3382993, 0.12988195, 0.108267635, 0.07244181, 0.086667664, 0.20383431, 0.18356943, 0.12639719, 0.08847797, 0.07500073, 0.06944024, 0.10883589, 0.2010424, 0.311054, 0.10910628, 0.048854824, 0.100422725, 0.14594004, 0.074504204, 0.07330239, 0.120831035, 0.06417202, 0.120831035, 0.19903311, 0.18051344, 0.14226134, 0.068000704, 0.28537333, 0.088079445, 0.31272593, 0.24532865, 0.26712537, 0.29217204, 0.3564662, 0.23498422, 0.15362592, 0.1703803, 0.1659183, 0.176165, 0.08655494, 0.22743575, 0.1208443, 0.029908128, 0.112730205, 0.20398888, 0.22229753, 0.2955012, 0.091938995, 0.060737267, 0.087870054, 0.084073074, 0.18923315, 0.201367, 0.04579684, 0.11932797, 0.12527451, 0.14116032, 0.20489767, 0.102696165, 0.2830313, 0.16937152, 0.13629325, 0.20250526, 0.13572311, 0.2010894, 0.08451614, 0.092287846, 0.25490743, 0.13909596, 0.0909305, 0.4452441, 0.23925708, 0.09631017, 0.07680977, 0.19986606, 0.15459888, 0.21196926, 0.028569484, 0.3268302, 0.19913498, 0.111281015, 0.17337199, 0.0933002, 0.073581345, 0.1296806, 0.10817327, 0.21582888, 0.043410104, 0.1356957, 0.17485902, 0.16162613, 0.08900968, 0.12913708, 0.14879267, 0.1109396, 0.1322715, 0.25754544, 0.15222052, 0.13590865, 0.05336622, 0.111886546, 0.059665903, 0.1293918, 0.2733778, 0.05374726, 0.123141825, 0.09515922, 0.20888934, 0.094733365, 0.0917838, 0.19896504, 0.2534523, 0.19040933, 0.29105824, 0.14810744, 0.0760717, 0.087380975, 0.21870235, 0.14304537, 0.15287401, 0.10461219, 0.06761834, 0.19343868, 0.10662661, 0.13191445, 0.12488579, 0.30929706, 0.07182732, 0.23273295, 0.16169767, 0.16572797, 0.16040146, 0.052570306, 0.107779264, 0.086441495, 0.2637899, 0.09768764, 0.22782713, 0.21593744, 0.12848844, 0.22674525, 0.12327061, 0.27329606, 0.26493832, 0.29732662, 0.10012593, 0.25201663, 0.07085322, 0.18831699, 0.054701366, 0.3964485, 0.08265394, 0.27347118, 0.119557634, 0.08481939, 0.41232583, 0.561618, 0.17709884, 0.15466675, 0.09769896, 0.15527551, 0.5859028, 0.40278414, 0.08096328, 0.112570085, 0.25281313, 0.20835006, 0.08092427, 0.47921395, 0.28492332, 0.17507608, 0.16889286, 0.07060895, 0.10974718, 0.12563631, 0.089802526, 0.089907974, 0.17667773, 0.09670507, 0.07967149, 0.23576616, 0.13722907, 0.11087141, 0.2968138, 0.23646472, 0.13188139, 0.2512602, 0.15061003, 0.1721826, 0.1992489, 0.08911959, 0.12550902, 0.13021834, 0.12013728, 0.3410868, 0.24178232, 0.1369471, 0.1958812, 0.06960491, 0.36442065, 0.19453953, 0.07055144, 0.07353294, 0.20399572, 0.2437213, 0.05886942, 0.16511671, 0.1743629, 0.20262994, 0.07337492, 0.1440552, 0.29592472, 0.24517235, 0.16005866, 0.3446218, 0.25949532, 0.085561655, 0.12272422, 0.18842256, 0.17228827, 0.12932445, 0.073759586, 0.31984115, 0.080332905, 0.1860379, 0.15004347, 0.13205266, 0.10156017, 0.074095465, 0.06793974, 0.2833249, 0.18042189, 0.14295211, 0.15214528, 0.15084106, 0.14276212, 0.15304871, 0.09401976, 0.10489824, 0.15359978, 0.17651823, 0.09249996, 0.41808504, 0.19232485, 0.1812598, 0.19991876, 0.24942173, 0.068827376, 0.21560417, 0.2722034, 0.1667586, 0.071560994, 0.25157002, 0.117020175, 0.09364679, 0.17244734, 0.22341563, 0.2424723, 0.067251146, 0.07573052, 0.11527318, 0.26215625, 0.14731379, 0.08000539, 0.24522144, 0.23398171, 0.095655456, 0.12168292, 0.0989483, 0.04666465, 0.2875653, 0.0893349, 0.09355463, 0.0745955, 0.24857439, 0.13615213, 0.073522754, 0.17802587, 0.08974754, 0.08256915, 0.10720919, 0.12084671, 0.19506158, 0.13583541, 0.11026746, 0.13790387, 0.4042908, 0.2418982, 0.46952954, 0.22982287, 0.0836949, 0.075374454, 0.10432429, 0.14428589, 0.052687287, 0.07022727, 0.2392553, 0.12601599, 0.15425803, 0.18615717, 0.112432554, 0.13735235, 0.0392244, 0.05361781, 0.11500789, 0.06649657, 0.055257756, 0.092137575, 0.10882474, 0.16114601, 0.17651112, 0.08588353, 0.2335538, 0.23587811, 0.24702536, 0.10268133, 0.07742726, 0.122062, 0.3554228, 0.14428999, 0.14760493, 0.06964642, 0.42716587, 0.24240851, 0.14621988, 0.11695029, 0.19198269, 0.23199609, 0.30345288, 0.098210976, 0.11764038, 0.098394096, 0.13805789, 0.08597158, 0.1773501, 0.11862864, 0.16268446, 0.17715356, 0.056346737, 0.12685989, 0.07552072, 0.112959564, 0.21897243, 0.061919257, 0.05386168, 0.13343124, 0.07184246, 0.24646497, 0.12761196, 0.14240026, 0.20268278, 0.0884401, 0.20509617, 0.103141695, 0.2981931, 0.24907275, 0.07678029, 0.28113008, 0.1101275, 0.14182784, 0.16096482, 0.092227496, 0.10458765, 0.09495128, 0.2385788, 0.2100435, 0.1000448, 0.20791967, 0.1209518, 0.3350045, 0.2013955, 0.12067606, 0.2288045, 0.36154765, 0.14095981, 0.18864985, 0.1695198, 0.08947868, 0.10724283, 0.30840835, 0.13154197, 0.20107669, 0.10533858, 0.10618959, 0.4988863, 0.3411665, 0.13629709, 0.051680937, 0.17727041, 0.1649136, 0.09652397, 0.29134193, 0.17618273, 0.15692292, 0.29667994, 0.122931115, 0.12797652, 0.22616151, 0.10426585, 0.36576894, 0.20207277, 0.18194756, 0.114377126, 0.03726364, 0.21589094, 0.3279235, 0.32324678, 0.10393397, 0.058809955, 0.050612833, 0.16347487, 0.08771654, 0.21610199, 0.11112025, 0.04128053, 0.11304786, 0.10806851, 0.3688549, 0.17593658, 0.28066957, 0.23733371, 0.07627087, 0.08106968, 0.08811278, 0.2618428, 0.13670386, 0.087240405, 0.13346983, 0.12904319, 0.07934278, 0.057107847, 0.05944162, 0.1469954, 0.12598519, 0.13643917, 0.12502018, 0.07418681, 0.09702098, 0.045453466, 0.06214093, 0.3305416, 0.5412816, 0.17115687, 0.10910222, 0.13369162, 0.33261877, 0.21098484, 0.38439122, 0.2684129, 0.08530948, 0.10147298, 0.21276979, 0.18904132, 0.19476901, 0.06701728, 0.050136704, 0.15812014, 0.26456583, 0.0952149, 0.10860644, 0.5413066, 0.05624256, 0.12699695, 0.18416286, 0.2077359, 0.20424694, 0.123110555, 0.2525863, 0.12628436, 0.12053922, 0.09453894, 0.09972529, 0.060715422, 0.13359874, 0.12661749, 0.29363745, 0.12363118, 0.18023561, 0.052204754, 0.21097669, 0.06780293, 0.12987635, 0.10651415, 0.12359877, 0.27672914, 0.058305692, 0.1329466, 0.3019686, 0.05048458, 0.118085384, 0.091505036, 0.17997389, 0.097942725, 0.1734791, 0.14343439, 0.2633756, 0.21412513, 0.0789409, 0.083686255, 0.12900098, 0.30566615, 0.13742419, 0.113845795, 0.11038383, 0.20013136, 0.11346916, 0.1070412, 0.3336879, 0.06417652, 0.1261118, 0.0928189, 0.13664328, 0.13984987, 0.1861543, 0.18854402, 0.07859197, 0.118810445, 0.10964644, 0.09262493, 0.16257982, 0.27662134, 0.25686106, 0.16498083, 0.16985856, 0.33686277, 0.3823682, 0.101801015, 0.060216613, 0.22138384, 0.20692854, 0.21550933, 0.20614445, 0.0486272, 0.18842834, 0.08923068, 0.19043897, 0.1350893, 0.18350758, 0.1126021, 0.16134062, 0.16304012, 0.11709285, 0.1946511, 0.23683313, 0.061564833, 0.58125997, 0.10226772, 0.3168343, 0.094368786, 0.07205114, 0.22258109, 0.19960453, 0.060887795, 0.13340166, 0.21607958, 0.17777108, 0.17071807, 0.35927594, 0.17086247, 0.4398082, 0.09969044, 0.26050794, 0.099874236, 0.19716404, 0.16239707, 0.092625156, 0.15801415, 0.11493139, 0.10665168, 0.13624245, 0.25347328, 0.14803806, 0.23246491, 0.21869572, 0.35101032, 0.13704045, 0.16759998, 0.286896, 0.1326723, 0.10969186, 0.09822157, 0.10199977, 0.17253524, 0.35528484, 0.18512857, 0.21593864, 0.1473605, 0.23116353, 0.11161392, 0.23231687, 0.15606695, 0.14268476, 0.23029216, 0.18479526, 0.098034374, 0.14734143, 0.19720122, 0.11469967, 0.26761755, 0.19658962, 0.11470759, 0.099359855, 0.16253912, 0.18498935, 0.31950107, 0.21820857, 0.2925725, 0.085247695, 0.08081223, 0.08230425, 0.13963601, 0.056980275, 0.06381734, 0.13837177, 0.22198088, 0.078756705, 0.15098545, 0.10132527, 0.12093528, 0.1455169, 0.21341333, 0.22984496, 0.15786576, 0.07296178, 0.04287353, 0.09154901, 0.07703814, 0.05822487, 0.22459272, 0.12234651, 0.09913222, 0.3679142, 0.16738416, 0.093823925, 0.13152987, 0.060871247, 0.17289017, 0.11770003, 0.12895815, 0.0803575, 0.2226077, 0.04320895, 0.08742145, 0.112193175, 0.10838442, 0.17465149, 0.14116916, 0.107088454, 0.30500305, 0.118162096, 0.15292186, 0.121518046, 0.14468102, 0.15082842, 0.10811008, 0.17804025, 0.29396904, 0.15338683, 0.087979, 0.2648897, 0.16107118, 0.09000326, 0.20708854, 0.10362029, 0.24408855, 0.088429764, 0.14981711, 0.28301644, 0.09018906, 0.132475, 0.29035825, 0.16519348, 0.16145545, 0.09144535, 0.07442299, 0.14670046, 0.068778925, 0.19482122, 0.17464875, 0.33936697, 0.10698374, 0.14153078, 0.1428322, 0.18255219, 0.09705954, 0.23968634, 0.07702593, 0.35495275, 0.18553436, 0.09528453, 0.13895623, 0.09981591, 0.16019198, 0.16931958, 0.05885978, 0.08701295, 0.1076117, 0.32986265, 0.17595255, 0.17117761, 0.16357583, 0.37339067, 0.13298845, 0.18534598, 0.22568502, 0.13794659, 0.12258452, 0.22671312, 0.08098606, 0.0724505, 0.18589725, 0.21640557, 0.17319784, 0.3967954, 0.06917298, 0.0891882, 0.29064235, 0.39108294, 0.28723714, 0.106617026, 0.124826156, 0.097314484, 0.097558975, 0.05639911, 0.124299765, 0.092672735, 0.12764049, 0.11460912, 0.13827324, 0.15707749, 0.19658372, 0.1672159, 0.04645188, 0.045004677, 0.14895716, 0.12700135, 0.09903246, 0.115926705, 0.14983845, 0.108612865, 0.20407088, 0.11862098, 0.06787744, 0.13602121, 0.095552385, 0.20647466, 0.14552799, 0.11342604, 0.18620004, 0.13896994, 0.24778925, 0.1313593, 0.19845556, 0.09712707, 0.14166713, 0.08655407, 0.10571333, 0.1139452, 0.165037, 0.06396253, 0.15148619, 0.13442932, 0.10442584, 0.1024456, 0.12317835, 0.23006596, 0.13828896, 0.10465675, 0.08361948, 0.14841141, 0.2618594, 0.13020337, 0.12577465, 0.09397035, 0.11802948, 0.101029046, 0.06940986, 0.21524364, 0.07356042, 0.25374997, 0.066813484, 0.121400796, 0.18077412, 0.09721404, 0.17675029, 0.19894257, 0.097616464, 0.091490485, 0.15877204, 0.25641555, 0.26257142, 0.15921444, 0.098375164, 0.1999502, 0.35229576, 0.19876648, 0.14615063, 0.26829025, 0.09532673, 0.10618782, 0.09649988, 0.18882269, 0.35506845, 0.08470877, 0.09067802, 0.14841713, 0.15699415, 0.07445747, 0.06634368, 0.23185053, 0.19196975, 0.18302318, 0.08863557, 0.25895938, 0.21155326, 0.113808855, 0.15501185, 0.18808383, 0.09901409, 0.12661918, 0.116182014, 0.06869565, 0.088637196, 0.33063498, 0.06348969, 0.118979655, 0.1697188, 0.098104894, 0.21140672, 0.20997445, 0.059571195, 0.08976044, 0.19587651, 0.10376422, 0.13250615, 0.18658069, 0.089802526, 0.26053935, 0.15163867, 0.14693557, 0.1360196, 0.09644145, 0.21100053, 0.1167632, 0.127366, 0.1433325, 0.16177204, 0.082908794, 0.097394556, 0.12674809, 0.12178514, 0.19502778, 0.16045909, 0.1439079, 0.32609677, 0.07358496, 0.32616255, 0.17255211, 0.2036105, 0.109128416, 0.06742226, 0.13344641, 0.15549216, 0.1518359, 0.13473491, 0.10725901, 0.15310082, 0.08849658, 0.08443678, 0.08952818, 0.19079274, 0.10283087, 0.13770373, 0.072303906, 0.06319531, 0.14005142, 0.14499322, 0.15914038, 0.105681874, 0.111256376, 0.101336315]\n",
      "0.5963022\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    b_test_seq, b_test_mask, b_test_y = batch\n",
    "    \n",
    "    # get predictions for test data\n",
    "    with torch.no_grad():\n",
    "        preds = model(b_test_seq.to(device), b_test_mask.to(device))\n",
    "        pred_prob = torch.exp(preds) #to_probability\n",
    "        pred_preds_np = pred_prob.detach().cpu().numpy()\n",
    "        pred_cls = pred_preds_np[:,1]\n",
    "        #print (pred_cls)\n",
    "        predictions.extend(list(pred_cls))\n",
    "\n",
    "print (predictions)\n",
    "pred_score = np.max(predictions)\n",
    "print (pred_score)\n",
    "#preds_score.append(pred_score)                        \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       153\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       153\n",
      "   macro avg       0.50      0.50      0.50       153\n",
      "weighted avg       1.00      0.99      1.00       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_y = np.argmax(pred_preds_np, axis = 1)\n",
    "print (test_y)\n",
    "print (preds_y)\n",
    "print(classification_report(test_y, preds_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36 (torch)",
   "language": "python",
   "name": "e36t11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
