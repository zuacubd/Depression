{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import torch\n",
    "#import transformers\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from load import *\n",
    "\n",
    "#from transformers import AutoModel, BertTokenizerFast\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "#from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "#                              TensorDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bert pre-trained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "#load the bert tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2018\n",
    "\n",
    "#input directory\n",
    "POS_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/train/positive_examples_anonymous_chunks/\"\n",
    "NEG_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/train/negative_examples_anonymous_chunks/\"\n",
    "TEST_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/test/\"\n",
    "\n",
    "#processed directory\n",
    "Processed_Train_DIR = \"/projets/sig/mullah/nlp/depression/data/processed/\"+str(year)+\"/train\"\n",
    "Processed_Test_DIR = \"/projets/sig/mullah/nlp/depression/data/processed/\"+str(year)+\"/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading training data and computing statistics\n",
    "chunks = [i for i in range(1, 11)]\n",
    "train_user_data = load_train_data(chunks, NEG_DIR, POS_DIR)\n",
    "print (\"Total users in trained set: {}\".format(len(train_user_data)))\n",
    "\n",
    "#statistics \n",
    "#users, label, only title, only text, both title + text, no title + text\n",
    "users = []\n",
    "labels = []\n",
    "writings_count = []\n",
    "total_writings = 0\n",
    "count_title = 0\n",
    "count_text = 0\n",
    "count_title_text = 0\n",
    "count_no_title_text = 0\n",
    "count_text_tokens = []\n",
    "count_title_tokens = []\n",
    "\n",
    "for idx in range(0, len(train_user_data)):\n",
    "    user_data = train_user_data[idx]\n",
    "    \n",
    "    users.append(user_data['uid'])\n",
    "    labels.append(user_data['class'])\n",
    "    writings = merge_writings(user_data)\n",
    "    writings_count.append(len(writings))\n",
    "\n",
    "    for m in writings:\n",
    "        if m[0] != '' and m[2] == '':\n",
    "            count_title = count_title + 1\n",
    "            count_title_tokens.append(len(m[0].split(' ')))\n",
    "        if m[0] =='' and m[2] != '':\n",
    "            count_text = count_text + 1\n",
    "            count_text_tokens.append(len(m[2].split(' ')))            \n",
    "        if m[0] != '' and m[2] != '':\n",
    "            count_title_text = count_title_text + 1\n",
    "            count_text_tokens.append(len(m[2].split(' ')))\n",
    "        if m[0] == '' and m[2] == '':\n",
    "            count_no_title_text = count_no_title_text + 1\n",
    "            count_title_tokens.append(len(m[0].split(' ')))\n",
    "\n",
    "print (\"Total users: {}\".format(len(users)))\n",
    "print (\"Total labels: {}\".format(len(labels)))\n",
    "print (\"Total writings: {}\".format(sum(writings_count)))\n",
    "\n",
    "print (\"#only title: {}, #only text: {}, #both title and text: {}, #no titel and text: {}\".format(count_title, count_text, count_title_text, count_no_title_text))\n",
    "avg_text_tokens = np.mean(count_text_tokens)\n",
    "print (\"Average token in text: {}\".format(avg_text_tokens))\n",
    "print (np.sum(np.asarray(count_text_tokens)>128))\n",
    "avg_title_tokens = np.mean(count_title_tokens)\n",
    "print (\"Average token in text: {}\".format(avg_title_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and sampling training data\n",
    "chunks = [i for i in range(1, 11)]\n",
    "train_user_data = load_train_data(chunks, NEG_DIR, POS_DIR)\n",
    "print (\"Total users in trained set: {}\".format(len(train_user_data)))\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(0, len(train_user_data)):\n",
    "    user_data = train_user_data[idx]\n",
    "\n",
    "    label = user_data['class']\n",
    "    label_code = 1 if label == 'p' else 0\n",
    "    writings = merge_writings(user_data)\n",
    "    \n",
    "    for m in writings:\n",
    "        if m[0] != '' and m[2] != '':\n",
    "            text = str(m[0]) + \" \" + str(m[2]) #just considering the title + text\n",
    "            labels.append(label_code)\n",
    "            texts.append(text)\n",
    "            \n",
    "        elif m[2] != '':\n",
    "            text = m[2] #just considering the text (not title)\n",
    "            labels.append(label_code)\n",
    "            texts.append(text)   \n",
    "                  \n",
    "print (\"Total labels: {}\".format(len(labels)))\n",
    "print (\"Total texts: {}\".format(len(texts)))\n",
    "\n",
    "data_labels_texts = list(zip(labels, texts))\n",
    "df_labels_texts = pd.DataFrame(data_labels_texts, columns=[\"labels\", \"texts\"])\n",
    "print (df_labels_texts.head())\n",
    "\n",
    "trained_datapath = os.path.join(Processed_Train_DIR, 'train_texts')\n",
    "df_labels_texts.to_csv(trained_datapath, index=False)\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and preparing testing data\n",
    "chunks = [i for i in range(1, 11)]\n",
    "test_user_data = load_test_data(chunks, year, TEST_DIR)\n",
    "print (\"Total users in test set: {}\".format(len(test_user_data)))\n",
    "\n",
    "for idx in range(0, len(test_user_data)):\n",
    "    user_data = test_user_data[idx]\n",
    "    \n",
    "    uid = user_data['uid']\n",
    "    print (\"User id: {}\".format(uid))\n",
    "    label = user_data['class']\n",
    "    label_code = 1 if label == 'p' else 0\n",
    "    \n",
    "    for chunk_data in user_data[\"data\"]:\n",
    "        chunk_number = chunk_data[\"chunk\"]\n",
    "        writings = chunk_data[\"writings\"]\n",
    "        \n",
    "        labels = []\n",
    "        texts = []\n",
    "        \n",
    "        for m in writings:\n",
    "            if m[0] != '' and m[2] != '':\n",
    "                text = str(m[0]) + \" \" + str(m[2]) #just considering the title + text\n",
    "                labels.append(label_code)\n",
    "                texts.append(text)\n",
    "            \n",
    "            elif m[0] != '':\n",
    "                text = m[0] #just considering the title\n",
    "                labels.append(label_code)\n",
    "                texts.append(text) \n",
    "            \n",
    "            elif m[2] != '':\n",
    "                text = m[2] #just considering the text (not title)\n",
    "                labels.append(label_code)\n",
    "                texts.append(text)      \n",
    "\n",
    "        print (\"Total labels: {}\".format(len(labels)))\n",
    "        print (\"Total texts: {}\".format(len(texts)))\n",
    "\n",
    "        data_labels_texts = list(zip(labels, texts))\n",
    "        df_labels_texts = pd.DataFrame(data_labels_texts, columns=[\"labels\", \"texts\"])\n",
    "        #print (df_labels_texts.head())\n",
    "\n",
    "        #chunk dir\n",
    "        if not os.path.exists(os.path.join(Processed_Test_DIR, \"chunk \"+str(chunk_number))):\n",
    "            os.makedirs(os.path.join(Processed_Test_DIR, \"chunk \"+str(chunk_number)))\n",
    "                     \n",
    "        user_test_datapath = os.path.join(Processed_Test_DIR, \"chunk \"+str(chunk_number), str(uid)+\".csv\")\n",
    "        print (user_test_datapath)\n",
    "        df_labels_texts.to_csv(user_test_datapath, index=False)\n",
    "    print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#combined test data cumulatively\n",
    "chunks = [i for i in range(1, 11)]\n",
    "\n",
    "for chunk_number in chunks:\n",
    "    chunk_processed_test_dir = os.path.join(Processed_Test_DIR, \"chunk \"+str(chunk_number))\n",
    "    chunk_cumulative_test_dir = os.path.join(Processed_Test_DIR, \"cumulative\", \"chunk \"+str(chunk_number))\n",
    "\n",
    "    if not os.path.exists(chunk_cumulative_test_dir):\n",
    "        os.makedirs(chunk_cumulative_test_dir)\n",
    "    \n",
    "    if chunk_number == 1:\n",
    "        for root, subdirs, files in os.walk(chunk_processed_test_dir):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                data_df = pd.read_csv(file_path)\n",
    "            \n",
    "                chunk_cumulative_test_datapath = os.path.join(chunk_cumulative_test_dir, filename)\n",
    "                data_df.to_csv(chunk_cumulative_test_datapath, index=False)\n",
    "    else:\n",
    "        for root, subdirs, files in os.walk(chunk_processed_test_dir):\n",
    "            for filename in files:\n",
    "                file_path2 = os.path.join(root, filename)\n",
    "                data_df2 = pd.read_csv(file_path2)\n",
    "                \n",
    "                file_path1 = os.path.join(Processed_Test_DIR, \"cumulative\", \"chunk \"+str(chunk_number-1), filename)\n",
    "                data_df1 = pd.read_csv(file_path1)\n",
    "            \n",
    "                data_df = data_df1.append(data_df2, ignore_index=True)\n",
    "            \n",
    "                chunk_cumulative_test_datapath = os.path.join(chunk_cumulative_test_dir, filename)\n",
    "                data_df.to_csv(chunk_cumulative_test_datapath, index=False)      \n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36 (torch)",
   "language": "python",
   "name": "e36t11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
