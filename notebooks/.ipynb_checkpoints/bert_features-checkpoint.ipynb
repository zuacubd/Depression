{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import xml.etree.ElementTree as et\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import pickle\n",
    "import gensim.models as g\n",
    "from gensim.utils import simple_preprocess\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2017\n",
    "POS_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/train/positive_examples_anonymous_chunks/\"\n",
    "NEG_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/train/negative_examples_anonymous_chunks/\"\n",
    "TEST_DIR = \"/projets/sig/mullah/nlp/depression/data/raw/\"+str(year)+\"/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                      eRisk data loading\n",
    "########################################################\n",
    "\n",
    "def get_user_data_from_xml_file(filePath, classval, chunkNumber = -1):\n",
    "\n",
    "\ttree = et.parse(filePath)\n",
    "\troot = tree.getroot()\n",
    "\tuserId = root.find('ID').text\n",
    "\tXMLWritings = root.findall('WRITING')\n",
    "\n",
    "\twritings = []\n",
    "\n",
    "\tfor w in XMLWritings:\n",
    "\t\ttitle = w.find('TITLE').text.replace('\\n', '. ').strip(' ')\n",
    "\t\tdate = datetime.strptime(w.find('DATE').text.strip(' '), \"%Y-%m-%d %H:%M:%S\")\n",
    "\t\ttext = w.find('TEXT').text.replace('\\n', '. ').strip(' ')\n",
    "\t\t\n",
    "\t\twritings.append([title, date, text])\n",
    "\n",
    "\tu = {\n",
    "\t\t\"chunk\" : chunkNumber,\n",
    "\t\t\"class\" : classval,\n",
    "\t\t\"uid\" : userId,\n",
    "\t\t\"data\": [{\"chunk\" : chunkNumber, \"writings\" : writings}]\n",
    "\t}\n",
    "\n",
    "\treturn u\n",
    " \n",
    "def get_users_data_from_chunks(chunks, classval='?', chunksDirPath=\"\"):\n",
    "\tusers_chunks_data = []\n",
    "\tnumchunks = 0\n",
    "\tfor i in chunks:\n",
    "\t\tchunkPath = chunksDirPath+\"chunk \"+str(i)+\"/\"\n",
    "\t\tfor file in os.listdir(chunkPath):\n",
    "\t\t\tif file.endswith('.xml'):\n",
    "\t\t\t\tusers_chunks_data.append(get_user_data_from_xml_file(chunkPath+file, classval, i))\n",
    "\t\tnumchunks += 1\n",
    "\n",
    "\t\t#print(\"CHUNK %d LOADED :) (%i entities)\"%(i,int(len(users_chunks_data)/numchunks)))\n",
    "\n",
    "\treturn get_merged_users_data(users_chunks_data)\n",
    "\n",
    "def get_merged_users_data(users_chunks_data):\n",
    "\tids = list(map(lambda e: e[\"uid\"], users_chunks_data))\n",
    "\n",
    "\tids = list(set(ids))\n",
    "\n",
    "\tmerged = []\n",
    "\n",
    "\tfor uid in ids:\n",
    "\t\tuser_chunks = get_user_chunks(uid, users_chunks_data)\n",
    "\t\tu = {\n",
    "\t\t\t\"uid\" : uid,\n",
    "\t\t\t\"class\" : user_chunks[0][\"class\"],\n",
    "\t\t\t\"data\" : []\n",
    "\t\t}\n",
    "\t\tfor ch in user_chunks:\n",
    "\t\t\tu[\"data\"] += ch[\"data\"]\n",
    "\t\tmerged.append(u)\n",
    "\n",
    "\treturn merged\n",
    "\n",
    "def get_user_chunks(uid, users_chunks_data):\n",
    "\tchunks_data = list(filter(lambda e: e[\"uid\"] == uid, users_chunks_data))\n",
    "\treturn chunks_data\n",
    "\n",
    "def load_train_neg_pos(chunks):\n",
    "\tnegs = get_users_data_from_chunks(chunks, 'n', NEG_DIR)\n",
    "\tposs = get_users_data_from_chunks(chunks, 'p', POS_DIR)\n",
    "\n",
    "\treturn negs, poss\n",
    "\n",
    "def load_train_data(chunks):\n",
    "\tnegs = get_users_data_from_chunks(chunks, 'n', NEG_DIR)\n",
    "\tposs = get_users_data_from_chunks(chunks, 'p', POS_DIR)\n",
    "\n",
    "\tpos_neg_users = negs + poss\n",
    "\n",
    "\treturn pos_neg_users\n",
    "\n",
    "\n",
    "def load_test_data(chunks):\n",
    "\tusers = get_users_data_from_chunks(chunks, '?', TEST_DIR)\n",
    "\n",
    "\treturn users\n",
    "\n",
    "\n",
    "# Attribute access\n",
    "def merge_writings(user):\n",
    "\twritings = []\n",
    "\n",
    "\tfor chunk in user[\"data\"]:\n",
    "\t\twritings += chunk[\"writings\"]\n",
    "\n",
    "\treturn writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "bert_layers = [-1, -2, -3, -4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function vectorizin data\n",
    "def vectorize(data):\n",
    "    vector_writing = []\n",
    "    for user in data:\n",
    "        m = merge_writings(user)\n",
    "        post_vector_list = []\n",
    "        for writing in m:\n",
    "            \n",
    "            if writing[0] ==\"\":\n",
    "                text = writing[2]\n",
    "            else:\n",
    "                if writing[2] !=\"\":\n",
    "                    text = writing[0]+\". \"+writing[2]\n",
    "                else:\n",
    "                    text = writing[0]\n",
    "            #input_ids = torch.tensor(tokenizer.encode(text,pad_to_max_length=True,truncation_strategy='only_first', max_length=512)).unsqueeze(0)  # Batch size 1\n",
    "            #with torch.no_grad():\n",
    "            #    outputs = model(input_ids)\n",
    "            #post_vector_list.append(outputs[0][0][0].detach().cpu().numpy())\n",
    "            #break\n",
    "            \n",
    "            encoded_input = tokenizer(text, pad_to_max_length=True,truncation=True, truncation_strategy='only_first', max_length=512, return_tensors='pt')\n",
    "            output = model(**encoded_input)\n",
    "            vectors = []\n",
    "            for ldx in bert_layers:\n",
    "                vectors.append(output[0][0][ldx].detach().cpu().numpy())\n",
    "            #print (vectors)\n",
    "            text_vect = np.mean(vectors, axis=0)\n",
    "            #print (text_vect)\n",
    "            post_vector_list.append(text_vect)\n",
    "        #print(len(np.mean(post_vector_list,axis=0)))\n",
    "        u={\n",
    "            \"uid\" : user[\"uid\"],\n",
    "            \"class\" : user[\"class\"],\n",
    "            \"vector\" : np.mean(post_vector_list,axis=0),\n",
    "            \"number_writing\" : len(m)\n",
    "\n",
    "        }\n",
    "        vector_writing.append(u)\n",
    "    return vector_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users in trainset: 486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/sig/mullah/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "chunks = [i for i in range(1, 11)]\n",
    "train_data = load_train_data(chunks)\n",
    "print (\"Total users in trainset: {}\".format(len(train_data)))\n",
    "\n",
    "#training dataset\n",
    "vector_train = vectorize(train_data)\n",
    "print (\"Vector trained prepared.\")\n",
    "\n",
    "for vect in vector_train:\n",
    "    chemin = \"/projets/depression/eRisk_vector_data/Bert/Base/depression_2017/train/pos/\"+vect[\"uid\"]+\".csv\"\n",
    "    \n",
    "    if vect[\"class\"] == 'n':\n",
    "        chemin = \"/projets/depression/eRisk_vector_data/Bert/Base/depression_2017/train/neg/\"+vect[\"uid\"]+\".csv\"\n",
    "    \n",
    "    with open(chemin, 'a+') as output:  \n",
    "        writer = csv.writer(output, lineterminator='\\n')\n",
    "        writer.writerow(vect[\"vector\"])\n",
    "\n",
    "#testing dataset\n",
    "for i in range(1,11):\n",
    "    test_data=load_test_data(range(1,i+1))\n",
    "    vector_test = vectorize(test_data)\n",
    "    for vect in vector_test:\n",
    "        chemin = \"/projets/depression/eRisk_vector_data/Bert/Base/depression_2017/test/\"+vect[\"uid\"]+\".csv\"\n",
    "        with open(chemin, 'a+') as output:  \n",
    "            writer = csv.writer(output, lineterminator='\\n')\n",
    "            writer.writerow(vect[\"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36 (torch)",
   "language": "python",
   "name": "e36t11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
